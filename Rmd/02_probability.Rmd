---
title: "Probability Theory"
author: "Matteo Courthoud"
type: book
weight: 2
date: 2021-10-29
bibliography: references.bib
link-citations: true
output: 
  ioslides_presentation:
    widescreen: true
    smaller: true
    transition: 0
    slide_level: 3
    css: custom.css
  ml_notebook: 
    toc: true
    toc_depth: 2
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    toc_collapsed: true
  md_document:
    variant: markdown_mmd
    preserve_yaml: true
---

## Probability

### Probability Space

A **probability space** is a triple $(\Omega, \mathcal A, P)$ where

- $\Omega$ is the sample space.
- $\mathcal A$ is the $\sigma$-algebra on $\Omega$.
- $P$ is a probability measure.

The **sample space** $\Omega$ is the space of all possible events.



### Sigma Algebra

A nonempty set (of subsets of $\Omega$) $\mathcal A \in 2^\Omega$ is a **sigma algebra** ($\sigma$-algebra) of $\Omega$ if the following conditions hold:

1. $\Omega \in \mathcal A$
2. If $A \in \mathcal A$, then $(\Omega - A) \in \mathcal A$
3. If $A_1, A_2, ... \in \mathcal A$, then $\bigcup _ {i=1}^{\infty} A_i \in \mathcal A$

> The smallest $\sigma$-algebra is $\lbrace \emptyset, \Omega \rbrace$ and the largest one is $2^\Omega$ (in cardinality terms).

Suppose $\Omega = \mathbb R$. Let $\mathcal{C} = \lbrace (a, b],-\infty \leq a<b<\infty \rbrace$. Then the **Borel** $\sigma$**- algebra** on $\mathbb R$ is defined by
$$
  \mathcal B (\mathbb R) = \sigma (\mathcal C)
$$



### Probability Measure

A **probability measure** $P$ is a set function with domain $\mathcal A$ and codomain $[0,1]$ such that

1. $P(A) \geq 0 \ \forall A \in \mathcal A$
2. $P$ is $\sigma$-additive: is $A_n \in \mathcal A$ are pairwise disjoint events ($A_j \cap A_k = \emptyset$ for $j \neq k$), then
$$
  P\left(\bigcup _ {n=1}^{\infty} A_{n} \right)=\sum _ {n=1}^{\infty} P\left(A_{n}\right)
$$
3. $P(\Omega) = 1$



### Properties

- $P\left(A^{c}\right)=1-P(A)$
- $P(\emptyset)=0$
- For $A, B \in \mathcal{A}$, $P(A \cup B)=P(A)+P(B)-P(A \cap B)$
- For $A, B \in \mathcal{A}$, if $A \subset B$ then $P(A) \leq P(B)$
- For $A_n \in \mathcal{A}$, $P \left(\cup _ {n=1}^\infty A_{n} \right) \leq \sum _ {n=1}^\infty P(A_n)$
- For $A_n \in \mathcal{A}$, if $A_n \uparrow A$ then $\lim _ {n \to \infty} P(A_n) = P(A)$



### Conditional Probability

Let $A, B \in \mathcal A$ and $P(B) > 0$, the **conditional probability** of $A$ given $B$ is
$$
  P(A | B)=\frac{P(A \cap B)}{P(B)}
$$

Two events $A$ and $B$ are **independent** if $P(A \cap B)=P(A) P(B)$.



### Theorems

**Theorem** (Law of Total Probability):
Let $(E_n) _ {n \geq 1}$ be a finite or countable partition of $\Omega$. Then, if $A \in \mathcal A$, 
$$
  P(A) = \sum_n P(A | E_n ) P(E_n)
$$

**Theorem** (Bayes Theorem):
Let $(E_n) _ {n \geq 1}$ be a finite or countable partition of $\Omega$, and suppose $P(A) > 0$. Then, 
$$
  P(E_n | A) = \frac{P(A | E_n) P(E_n)}{\sum_m P(A | E_m) P(E_m)}
$$



## Random Variables

### Definition

A **random variable** $X$ on a probability space $(\Omega,\mathcal A, P)$ is a (measurable) mapping $X : \Omega \to \mathbb{R}$ such that
$$
  \forall B \in \mathcal{B}(\mathbb{R}), \quad X^{-1}(B) \in \mathcal{A}
$$

> The measurability condition states that the inverse image is a measurable set of $\Omega$ i.e. $X^{-1}(B) \in \mathcal A$. This is essential since probabilities are defined only on $\mathcal A$.



### Distribution Function

Let $X$ be a real valued random variable. The **distribution function** (also called cumulative distribution function) of $X$, commonly denoted $F_X(x)$ is defined by
$$
	F_X(x) = \Pr(X \leq x)
$$

> Properties
>
- $F$ is monotone non-decreasing
- $F$ is right continuous
- $\lim _ {x \to - \infty} F(x)=0$ and $\lim _ {x \to + \infty} F(x)=1$

The random variables $(X_1, .. , X_n)$ are independent if and only if
$$
  F _ {(X_1, ... , X_n)} (x) = \prod _ {i=1}^n F_{X_i} (x_i) \quad \forall x \in \mathbb R^n
$$



### Density Function

Let $X$ be a real valued random variable. $X$ has a **probability density function** if there exists $f_X(x)$ such that for all measurable $A \subset \mathbb{R}$,
$$
	 	P(X \in A) = \int_A f_X(x) \mathrm{d} x
$$





## Moments

### Expected Value

The **expected value** of a random variable, when it exists, is given by
$$
		\mathbb{E}[ X ] = \int_ \Omega X(\omega) \mathrm{d} P
$$
When $X$ has a density, then
$$
\mathbb{E} [ X ] = \int_ \mathbb{R} x f_X (x) \mathrm{d} x = \int _ \mathbb{R} x \mathrm{d} F_X (x)
$$

The **empirical expectation** (or **sample average**) is given by
$$
		\mathbb{E}_n [z_i] = \frac{1}{n} \sum _ {i=1}^N z_i 
$$



### Variance and Covariance

The **covariance** of two random variables $X$, $Y$ defined on $\Omega$ is 
$$
	Cov(X, Y ) = \mathbb{E}[ (X - \mathbb{E}[ X ]) (Y - \mathbb{E}[ Y ]) ]  = \mathbb{E}[XY ] - \mathbb{E}[ X ]E[ Y ]
$$
In vector notation, $Cov(X, Y) = \mathbb{E}[XY']  - \mathbb{E}[ X ]\mathbb{E}[Y']$.

The **variance** of a random variable $X$, when it exists, is given by
$$
	Var(X) = \mathbb{E}[ (X - \mathbb{E}[ X ])^2 ] = \mathbb{E}[X^2] - \mathbb{E}[ X ]^2
$$
In vector notation,  $Var(X) = \mathbb{E}[XX'] - \mathbb{E}[ X ]\mathbb{E}[X']$.



### Properties

Let $X, Y, Z, T \in \mathcal{L}^{2}$ and $a, b, c, d \in \mathbb{R}$

- $Cov(X, X) = Var(X)$
- $Cov(X, Y) = Cov(Y, X)$
- $Cov(aX + b, Y) = a \ Cov(X,Y)$
- $Cov(X+Z, Y) = Cov(X,Y) + Cov(Z,Y)$
- $Cov(aX + bZ, cY + dT) = ac * Cov(X,Y) + ad * Cov(X,T) + bc * Cov(Z,Y) + bd * Cov(Z,T)$

Let $X, Y \in \mathcal L^1$ be independent. Then, $\mathbb E[XY] = \mathbb E[ X ]E[ Y ]$.

If $X$ and $Y$ are independent, then $Cov(X,Y) = 0$.

> Note that the converse does not hold: $Cov(X,Y) = 0 \not \to X \perp Y$.



### Sample Variance

The **sample variance** is given by
$$
	Var_n (z_i) = \frac{1}{n} \sum _ {i=1}^N (z_i - \bar{z})^2 
$$
where $\bar{z_i} = \mathbb{E}_n [z_i] = \frac{1}{n} \sum _ {i=1}^N z_i$.



### Finite Sample Bias Theorem

**Theorem**: 
The expected sample variance $\mathbb{E} [\sigma^2_n] =  \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^N \left(y_i - \mathbb{E}_n[ Y ] \right)^2 \right]$ gives an estimate of the population variance that is biased by a factor of $\frac{1}{n}$ and is therefore referred to as **biased sample variance**.
	
**Proof**:
$$
  \begin{aligned}
  &\mathbb{E}[\sigma^2_n] =  \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^n \left( y_i - \mathbb{E}_n [ Y ] \right)^2 \right] = 
  \newline 
  &= \mathbb{E} \left[ \frac{1}{n} \sum _ {i=1}^n \left( y_i - \frac{1}{n} \sum _ {i=1}^n y_i \right )^2 \right] = 
	\newline 
	&= \frac{1}{n} \sum _ {i=1}^n \mathbb{E} \left[ y_i^2 - \frac{2}{n} y_i \sum _ {j=1}^n y_j + \frac{1}{n^2} \sum _ {j=1}^n y_j \sum _ {k=1}^{n}y_k  \right] = 
	\newline 
	&= \frac{1}{n} \sum _ {i=1}^n  \left[ \frac{n-2}{n} \mathbb{E}[y_i^2]  - \frac{2}{n} \sum _ {j\neq i} \mathbb{E}[y_i y_j] + \frac{1}{n^2} \sum _ {j=1}^n \sum _ {k\neq j} \mathbb{E}[y_j y_k] + \frac{1}{n^2} \sum _ {j=1}^n \mathbb{E}[y_j^2] \right] = 
	\newline 
	&= \frac{1}{n} \sum _ {i=1}^n  \left[ \frac{n-2}{n}(\mu^2 + \sigma^2) - \frac{2}{n} (n-1) \mu^2 + \frac{1}{n^2} n(n-1)\mu^2 + \frac{1}{n^2} n (\mu^2 + \sigma^2)]\right] =
	\newline 
	&= \frac{n-1}{n} \sigma^2
	\end{aligned}
$$
$$\tag*{$\blacksquare$}$$

### Inequalities

- **Triangle Inequality**: if $\mathbb{E} [ X ] < \infty$, then 
 $$|\mathbb{E} [ X ] | \leq \mathbb{E} [|X|]$$
- **Markov's Inequality**: if $\mathbb{E}[ X ] < \infty$, then 
 $$ \Pr(|X| > t) \leq \frac{1}{t} \mathbb{E}[|X|] $$
- **Chebyshev's Inequality**: if $\mathbb{E}[X^2] < \infty$,  then 
 $$ \Pr(|X- \mu|> t \sigma) \leq \frac{1}{t^2}\Leftrightarrow \Pr(|X- \mu|> t ) \leq \frac{\sigma^2}{t^2} $$
- **Cauchy-Schwarz's Inequality**:
 $$ \mathbb{E} [|XY|] \leq \sqrt{\mathbb{E}[X^2] \mathbb{E}[Y^2]} $$
- **Minkowski Inequality**: 
 $$ \left( \sum _ {k=1}^n | x_k + y_k |^p \right) ^ {\frac{1}{p}} \leq \left( \sum _ {k=1}^n | x_k |^p \right) ^ {\frac{1}{p}} + \left( \sum _ {k=1}^n | y_k | ^p \right) ^ { \frac{1}{p} } $$
- **Jensen's Inequality**: if $g( \cdot)$ is concave (e.g. logarithmic function), then 
 $$ \mathbb{E}[g(x)] \leq g(\mathbb{E}[ X ]) $$ 
  Similarly, if $g(\cdot)$ is convex (e.g. exponential function), then 
  $$ \mathbb{E}[g(x)] \geq g(\mathbb{E}[ X ]) $$





### Theorems

**Theorem**: 
Law of Iterated Expectations
$$
  \mathbb{E}(Y) = \mathbb{E}_X [\mathbb{E}(Y|X)]
$$
**Theorem**: 
Law of Total Variance
$$
		Var(Y) = Var_X (\mathbb{E}[Y |X]) + \mathbb{E}_X [Var(Y|X)] 
$$

Useful distributional results:

1. $\chi^2_q \sim \sum _ {i=1}^q Z_i^2$ where $Z_i \sim N(0,1)$
2. $F(n_1 , n_2) \sim \frac{\chi^2 _ {n_1} / n_1}{\chi^2 _ {n_2}/n_2}$
3. $t_n \sim \frac{Z}{\sqrt{\chi^2 _ n}/n }$

> The $t$ distribution is approximately standard normal but has heavier tails. The approximation is good for $n \geq 30$: $t_{n\geq 30} \sim N(0,1)$ 

## Statistical Models

### Definition

A **statistical model** is a set of probability distributions. More precisely, a **statistical model over data** $D \in \mathcal{D}$ is a set of probability distribution over datasets $D$ which takes values in $\mathcal{D}$.

Suppose you have regression data $\lbrace \mathbb{x}_i , y_i \rbrace _ {i=1}^N$ with $\mathbb{x}_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$. The statistical model is

$$
 \Big\lbrace   P : y_i =  f(\mathbb{x}_i) + \varepsilon_i, \ x_i \sim F_x , \ \varepsilon_i \sim F _\varepsilon , \ \varepsilon_i \perp \mathbb{x}_i , \ f \in C^2 (\mathbb{R}^p) \Big\rbrace 
$$

> In words: the statistical model is the set of distributions $P$ such that an additive decomposition of $y_i$ as $f(\mathbb{x}_i) + \varepsilon_i$ exists for some $\mathbb{x}_i$; where $f$ is twice continuously differentiable.

A statistical model parameterized by $\theta \in \Theta$ is **well specified** if the data generating process corresponds to some $\theta_0$ and $\theta_0 \in \Theta$. Otherwise, the statistical model is **misspecified**.

A statistical model can be parametrized as $\mathcal{F} = \lbrace   P_\theta \rbrace _ {\lbrace \theta \in \Theta \rbrace }$. 



### Categories of Statistical Models

- **Parametric**: the stochastic features of the model are completly specified up to a finite dimensional parameter: $\lbrace   P_\theta \rbrace _ { \lbrace   \theta \in \Theta \rbrace }$ with $\Theta \subseteq \mathbb{R}^k, k<\infty$;
- **Semiparametric**: it is a partially specified model, e.g.,  $\lbrace   P_\theta \rbrace  _ { \lbrace   \theta \in \Theta, \gamma \in \Gamma \rbrace }$ with $\Theta$ of finite dimension and $\Gamma$ of infinite dimension;
- **Non parametric**: there is no finite dimensional component of the model. 



### Linear Model

In a **linear model data** are given by $D_n = \lbrace   (y_i, x _ {i1}, \dots, x _ {ik}) \rbrace _ {i=1}^n \in \mathcal{D}$ where:

- $D_n$ are the observed data;
- $y_i$ is the dependent variable;
- $x_ {i1}, \dots, x_ {ik}$ are the regressors including a constant.



### Estimation and Inference

Let $\mathcal{D}$ be the set of possible data realizations. Let $D \in \mathcal{D}$ be your data. Let $\mathcal{F}$ be a statistical model indexed by $\theta$. Let $\nu$ be a functional $\mathcal{F} \to \mathbb{R}$. Let $\alpha > 0$ be a small tolerance. An **estimator** is a map 
$$
		\mathcal{D} \to \mathcal{F} \quad , \quad  D \mapsto \hat{\theta} \qquad \text{ or } \qquad \mathcal{D} \to \mathbb{R} \quad , \quad D \mapsto \hat{\nu}
$$

Statistical **inference** is a map into subsets of $\mathcal{F}$ given by
$$
  \mathcal{D} \to \mathcal{G} \subseteq \mathcal{F}: \min _ \theta P_\theta (\mathcal{G} | \theta \in \mathcal{G}) \geq 1-\alpha  \qquad \text{ or } \qquad \mathcal{D} \to A \subseteq \mathbb{R}: \min _ \theta P_\theta (A | \nu(\theta) \in A) \geq 1-\alpha
$$



### DGP

A **data generating process** (DGP) is a single statistical distribution over $\mathcal{D}$.

Suppose you have a statistical model parametrized by $\theta$ and an estimator $\hat{\theta}$. The **bias** of $\hat{\theta}$ relative to $\theta$ is given by
$$
	Bias _ {\theta} (\hat{\theta}) = \mathbb{E} _ {x|\theta} [\hat{\theta} ] - \theta = \mathbb{E} _ {x|\theta} [\hat{\theta} - \theta]
$$

Let $\hat{\theta}$ be an estimator for $\theta_0$. We say $\hat{\theta}$ is an **unbiased** estimator for $\theta$ if $\mathbb{E}[\hat{\theta}] = \theta_0$.

